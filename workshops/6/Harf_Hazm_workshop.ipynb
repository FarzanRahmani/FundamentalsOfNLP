{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# setting up files, packages, and libraries"
      ],
      "metadata": {
        "id": "Outz35NzwnMy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Required Libraries"
      ],
      "metadata": {
        "id": "oVaH9m7RchXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hazm\n",
        "!pip install evaluate\n",
        "!pip install jiwer\n",
        "!pip install mega.py"
      ],
      "metadata": {
        "id": "ctIoZ0C9cXKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "restart session"
      ],
      "metadata": {
        "id": "0aNrcCOZdCdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Restart Session\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "YXJGDDxkdCEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "import libraries"
      ],
      "metadata": {
        "id": "ww6-5kewcoQJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "yQPDpS9BcRZI"
      },
      "outputs": [],
      "source": [
        "import hazm\n",
        "import evaluate\n",
        "from mega import Mega\n",
        "from collections import Counter\n",
        "import jiwer\n",
        "from jiwer import wer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "upload files"
      ],
      "metadata": {
        "id": "RvsmU93YnoFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKjrBhsLnqzq",
        "outputId": "936c9c13-8eea-434c-e9cc-7acf523d0ebd"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " fasttext_model.zip\n",
            " fasttext_skipgram_300.bin\n",
            " fasttext_skipgram_300.vec\n",
            " HarfOutp.txt\n",
            "'[Persian] آلن تورینگ پدر هوش مصنوعی و بزرگترین کد شکن تاریخ [DownSub.com].txt'\n",
            " pos_tagger.model\n",
            " \u001b[0m\u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q4"
      ],
      "metadata": {
        "id": "2GiQuRiXcYiN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize normalizer"
      ],
      "metadata": {
        "id": "rANTvUKxn3S6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalizer = hazm.Normalizer()\n",
        "normalizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKbmZHzCdKKB",
        "outputId": "6485db74-e938-48c4-a6b2-0ba52cee4cb1"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<hazm.normalizer.Normalizer at 0x79a816febeb0>"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to read and normalize text from a file"
      ],
      "metadata": {
        "id": "zXV9x2BOn5vK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_and_normalize(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "    normalized_text = normalizer.normalize(text)\n",
        "    return normalized_text"
      ],
      "metadata": {
        "id": "faa7dqSbdNPi"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read and normalize the prediction and reference texts"
      ],
      "metadata": {
        "id": "15_QqmhGn71q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_text = read_and_normalize('HarfOutp.txt')\n",
        "reference_text = read_and_normalize('[Persian] آلن تورینگ پدر هوش مصنوعی و بزرگترین کد شکن تاریخ [DownSub.com].txt')\n",
        "\n",
        "print(\">>> prediction_text[:85]:\")\n",
        "print(f\"\\t {prediction_text[:85]}\")\n",
        "print(\">>> reference_text[:85]:\")\n",
        "print(f\"\\t {reference_text[:85]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d90KrtgheQy2",
        "outputId": "d8de8903-3138-42c9-edb4-9164f0d4e4cf"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> prediction_text[:85]:\n",
            "\t . در دوران جنگ جهانی دوم، یکی از برگ برنده‌های متفقین رمزگشایی از پیام‌های سری نازی‌ه\n",
            ">>> reference_text[:85]:\n",
            "\t در دوران جنگ جهانی دوم یکی از برگه\n",
            "برنده‌های متفقین رمزگشایی از پیام‌های\n",
            "\n",
            "سری نازی‌ها\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Align the sequences using jiwer"
      ],
      "metadata": {
        "id": "W30H2g8SoCRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformation = jiwer.Compose([\n",
        "    jiwer.RemovePunctuation(),\n",
        "    jiwer.RemoveMultipleSpaces(),\n",
        "    jiwer.Strip(),\n",
        "    jiwer.ToLowerCase(),\n",
        "    jiwer.RemoveEmptyStrings()\n",
        "])\n",
        "\n",
        "reference_text = reference_text.replace(\"\\n\", \" \") # remove new lines\n",
        "\n",
        "# Transform the text\n",
        "transformed_prediction = transformation(prediction_text)\n",
        "transformed_reference = transformation(reference_text)\n",
        "\n",
        "print(\">>> transformed_prediction[:85]:\")\n",
        "print(f\"\\t {transformed_prediction[:85]}\")\n",
        "print(\">>> transformed_reference[:85]:\")\n",
        "print(f\"\\t {transformed_reference[:85]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WbKBxpk4eM_",
        "outputId": "412e28ac-c036-42b6-8c6b-f993cc993855"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> transformed_prediction[:85]:\n",
            "\t در دوران جنگ جهانی دوم یکی از برگ برنده‌های متفقین رمزگشایی از پیام‌های سری نازی‌ها ب\n",
            ">>> transformed_reference[:85]:\n",
            "\t در دوران جنگ جهانی دوم یکی از برگه برنده‌های متفقین رمزگشایی از پیام‌های سری نازی‌ها \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate WER using the evaluate library\n",
        "wer_metric = evaluate.load(\"wer\")\n",
        "\n",
        "# truncation_threshold = min(len(pred_words), len(ref_words))\n",
        "# wer = wer_metric.compute(predictions=pred_words[:truncation_threshold], references=ref_words[:truncation_threshold])\n",
        "# wer = wer_metric.compute(predictions=[prediction_text], references=[reference_text])\n",
        "wer = wer_metric.compute(predictions=[transformed_prediction], references=[transformed_reference])\n",
        "\n",
        "# Print the WER\n",
        "print(f\">>> Word Error Rate (WER): {wer:.6f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gYKt1yXdRl6",
        "outputId": "5b0c4d08-a165-4e73-cbe9-6b7f63dfc4bd"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Word Error Rate (WER): 0.227290\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate CER using the evaluate library\n",
        "cer_metric = evaluate.load(\"cer\")\n",
        "\n",
        "# truncation_threshold = min(len(pred_words), len(ref_words))\n",
        "# cer = cer_metric.compute(predictions=pred_words[:truncation_threshold], references=ref_words[:truncation_threshold])\n",
        "# cer = cer_metric.compute(predictions=[prediction_text], references=[reference_text])\n",
        "cer = cer_metric.compute(predictions=[transformed_prediction], references=[transformed_reference])\n",
        "\n",
        "\n",
        "# Print the CER\n",
        "print(f\">>> Character Error Rate (CER): {cer:.6f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJ69k1oucq0x",
        "outputId": "e354ed3e-88ff-454e-944c-fc9b5663fb20"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Character Error Rate (CER): 0.100094\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q5"
      ],
      "metadata": {
        "id": "YL9V_aDCil29"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "download model"
      ],
      "metadata": {
        "id": "bt7j6QrrteNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1Q3JK4NVUC2t5QT63aDiVrCRBV225E_B3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNic4WM-tTJU",
        "outputId": "9e47f8ec-04b3-40f4-9b3a-65ac928c094b"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Q3JK4NVUC2t5QT63aDiVrCRBV225E_B3\n",
            "To: /content/pos_tagger.model\n",
            "100% 19.2M/19.2M [00:00<00:00, 89.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize normalizer and POS tagger"
      ],
      "metadata": {
        "id": "dok02990oLxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalizer = hazm.Normalizer()\n",
        "\n",
        "tagger = hazm.POSTagger(model='pos_tagger.model')\n",
        "# tagger = hazm.POSTagger(model=hazm.utils.download_model('postagger')) # should be implemented by publishers !!!"
      ],
      "metadata": {
        "id": "-PUMfJbXoNyr"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to read, normalize, and POS tag text from a file"
      ],
      "metadata": {
        "id": "BeOxAiP2oRZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_normalize_and_tag(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "    normalized_text = normalizer.normalize(text)\n",
        "    words = hazm.word_tokenize(normalized_text)\n",
        "    pos_tags = tagger.tag(words)\n",
        "    return pos_tags"
      ],
      "metadata": {
        "id": "pUMg5W2_oTw6"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to count verbs and adverbs in POS tagged text"
      ],
      "metadata": {
        "id": "-fAzF_LLoVoa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_verbs_and_adverbs(pos_tags):\n",
        "    verbs = sum(1 for word, tag in pos_tags if tag.startswith('V'))\n",
        "    adverbs = sum(1 for word, tag in pos_tags if tag == 'ADV')\n",
        "    return verbs, adverbs"
      ],
      "metadata": {
        "id": "iZ0EGPpxoX5S"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Process the prediction and reference files"
      ],
      "metadata": {
        "id": "juR-lhSIoYQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_pos_tags = read_normalize_and_tag('HarfOutp.txt')\n",
        "reference_pos_tags = read_normalize_and_tag('[Persian] آلن تورینگ پدر هوش مصنوعی و بزرگترین کد شکن تاریخ [DownSub.com].txt')"
      ],
      "metadata": {
        "id": "ALokHTzboamS"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Count verbs and adverbs"
      ],
      "metadata": {
        "id": "LAlim9xVoeCj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_verbs, prediction_adverbs = count_verbs_and_adverbs(prediction_pos_tags)\n",
        "reference_verbs, reference_adverbs = count_verbs_and_adverbs(reference_pos_tags)\n",
        "\n",
        "print(f\">>> Prediction \\n\\t Verbs: {prediction_verbs}, Adverbs: {prediction_adverbs}\")\n",
        "print(f\">>> Reference \\n\\t Verbs: {reference_verbs}, Adverbs: {reference_adverbs}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66RKrUy7ocpM",
        "outputId": "9f2d8b23-7e54-4936-a9c1-8a84064f753a"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Prediction \n",
            "\t Verbs: 294, Adverbs: 78\n",
            ">>> Reference \n",
            "\t Verbs: 290, Adverbs: 74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q6"
      ],
      "metadata": {
        "id": "9Q0c-7M_iqHl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize normalizer, POS tagger, and lemmatizer"
      ],
      "metadata": {
        "id": "a0EYY0D2oyOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !gdown 1Q3JK4NVUC2t5QT63aDiVrCRBV225E_B3 # previously we did in #Q5\n",
        "\n",
        "normalizer = hazm.Normalizer()\n",
        "tagger = hazm.POSTagger(model='pos_tagger.model')\n",
        "# stemmer = hazm.Stemmer()\n",
        "lemmatizer = hazm.Lemmatizer()"
      ],
      "metadata": {
        "id": "3R19wMdAo0mC"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to read, normalize, and POS tag text from a file"
      ],
      "metadata": {
        "id": "Pl8lkEIQo2nq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_normalize_and_tag(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "    normalized_text = normalizer.normalize(text)\n",
        "    words = hazm.word_tokenize(normalized_text)\n",
        "    pos_tags = tagger.tag(words)\n",
        "    return pos_tags"
      ],
      "metadata": {
        "id": "NEaprmGso3KK"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to extract and count verb stems"
      ],
      "metadata": {
        "id": "dIM2BlTxo5aK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_and_count_verbs(pos_tags):\n",
        "    verbs = [word for word, tag in pos_tags if tag.startswith('V')]\n",
        "\n",
        "    # verb_stems = [stemmer.stem(verb) for verb in verbs] # رفتم -> رف\n",
        "    # verb_counts = Counter(verb_stems) # رفتم -> رفت#رو\n",
        "    verb_lemmas = [lemmatizer.lemmatize(verb) for verb in verbs]\n",
        "    verb_counts = Counter(verb_lemmas)\n",
        "\n",
        "    most_common_verb = verb_counts.most_common(1)[0]\n",
        "    return most_common_verb"
      ],
      "metadata": {
        "id": "xOJ5X4flo6sq"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Process the prediction and reference files"
      ],
      "metadata": {
        "id": "8cBI7Mlqo9tS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_pos_tags = read_normalize_and_tag('HarfOutp.txt')\n",
        "reference_pos_tags = read_normalize_and_tag('[Persian] آلن تورینگ پدر هوش مصنوعی و بزرگترین کد شکن تاریخ [DownSub.com].txt')"
      ],
      "metadata": {
        "id": "WUgAlCq6o-dy"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract and count verbs"
      ],
      "metadata": {
        "id": "38PWPFSxpBBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_most_common_verb = extract_and_count_verbs(prediction_pos_tags)\n",
        "reference_most_common_verb = extract_and_count_verbs(reference_pos_tags)"
      ],
      "metadata": {
        "id": "OxRJIWM-pDwq"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the most common verbs"
      ],
      "metadata": {
        "id": "1qk5OolmpG0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\">>> Prediction \\n\\t Most common verb root: '{prediction_most_common_verb[0]}', Count: {prediction_most_common_verb[1]}\")\n",
        "print(f\">>> Reference.txt \\n\\t Most common verb root: '{reference_most_common_verb[0]}', Count: {reference_most_common_verb[1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuluJTByirOE",
        "outputId": "bf1c7e33-a30d-4c59-ca24-fcc4e480b025"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Prediction \n",
            "\t Most common verb root: 'کرد#کن', Count: 45\n",
            ">>> Reference.txt \n",
            "\t Most common verb root: 'کرد#کن', Count: 36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q7"
      ],
      "metadata": {
        "id": "g7W87sW2ir1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "downolad fasttext_skipgram_300.bin"
      ],
      "metadata": {
        "id": "4Sn7LpL0kdgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mega = Mega()\n",
        "# m = mega.login(email, password)\n",
        "# # login using a temporary anonymous account\n",
        "m = mega.login()\n",
        "m.download_url('https://mega.nz/file/GqZUlbpS#XRYP5FHbPK2LnLZ8IExrhrw3ZQ-jclNSVCz59uEhrxY')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8JEfD7ukhHC",
        "outputId": "7c034509-625b-4865-fc6a-7645d12fd375"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('fasttext_model.zip')"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "unzip model"
      ],
      "metadata": {
        "id": "OEkal18KsnTR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip fasttext_model.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Qv3yliWpmWB",
        "outputId": "f9a0ddb8-c555-4e5c-99c7-f1c34973edd3"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  fasttext_model.zip\n",
            "  inflating: fasttext_skipgram_300.bin  \n",
            "  inflating: fasttext_skipgram_300.vec  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "test on a single example"
      ],
      "metadata": {
        "id": "Lg9DyMnGspuX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_embedding = hazm.WordEmbedding(model_type = 'fasttext', model_path = 'fasttext_skipgram_300.bin')\n",
        "word_embedding.doesnt_match(['سلام' ,'درود' ,'خداحافظ' ,'پنجره'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "XSit2EL6itc7",
        "outputId": "c829e130-dd6d-45e1-aaaa-e8089f2168e8"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'پنجره'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "write desired function in question"
      ],
      "metadata": {
        "id": "hkpSa9hRssnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_unrelated_words(_words_lists, _word_embedding):\n",
        "    unrelated_words = []\n",
        "    for words_list in _words_lists:\n",
        "        unrelated_word = _word_embedding.doesnt_match(words_list)\n",
        "        unrelated_words.append(unrelated_word)\n",
        "    return unrelated_words"
      ],
      "metadata": {
        "id": "oQPrm9Pbk2Dr"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "test desired function"
      ],
      "metadata": {
        "id": "IlLK_56PsxjI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_word_lists = [\n",
        "        ['کتاب', 'مدرسه', 'ماشین', 'دانشگاه', 'مداد'],\n",
        "        ['گل', 'درخت', 'چمن', 'صندلی', 'بوته'],\n",
        "        ['سلام' ,'درود' ,'خداحافظ' ,'پنجره', 'بدرورد'],\n",
        "        ['ساعت' ,'پلنگ' ,'شیر', 'ببر', 'سگ']\n",
        "    ]\n",
        "\n",
        "\n",
        "unrelated_words = find_unrelated_words(list_of_word_lists, word_embedding)\n",
        "print(f\">>> Words lists:\")\n",
        "for list_of_word_list in list_of_word_lists:\n",
        "    print(f\"\\t {list_of_word_list}\")\n",
        "print()\n",
        "print(f\">>> Unrelated words in each list:\")\n",
        "for unrelated_word in unrelated_words:\n",
        "    print(f\"\\t {unrelated_word}\")\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lihXV580lvAM",
        "outputId": "c6cab36e-9b3c-4fc8-acd7-e3a4470b205b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Words lists:\n",
            "\t ['کتاب', 'مدرسه', 'ماشین', 'دانشگاه', 'مداد']\n",
            "\t ['گل', 'درخت', 'چمن', 'صندلی', 'بوته']\n",
            "\t ['سلام', 'درود', 'خداحافظ', 'پنجره', 'بدرورد']\n",
            "\t ['ساعت', 'پلنگ', 'شیر', 'ببر', 'سگ']\n",
            "\n",
            ">>> Unrelated words in each list:\n",
            "\t ماشین\n",
            "\t صندلی\n",
            "\t پنجره\n",
            "\t ساعت\n",
            "\n"
          ]
        }
      ]
    }
  ]
}