# -*- coding: utf-8 -*-
"""simple_vqa_WS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TKWdPZRiEO6wayCzwcZZu8A4hgF2FmTl
"""

# from google.colab import drive
# drive.mount('/content/drive')

# %cd './drive/MyDrive/simple_vqa'

!wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Annotations_Train_mscoco.zip && unzip v2_Annotations_Train_mscoco && \
wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Train_mscoco.zip && unzip v2_Questions_Train_mscoco

!pip install transformers

import os
import ssl
ssl._create_default_https_context = ssl._create_unverified_context
import json
import copy
import glob
import torch
import random
import operator
import numpy as np
import urllib.request
import matplotlib.pyplot as plt


from PIL import Image
from tqdm import tqdm
from google.colab import files
from numpy.random import choice
from sklearn.manifold import TSNE
from collections import Counter, defaultdict
import random
import pandas as pd

import torch
import json
import csv
from torch.utils.data import Dataset
from torchvision.transforms import *
from PIL import Image
import os
import numpy as np
from tqdm.notebook import tqdm
import pickle
from google.colab import files

from transformers import DistilBertTokenizer, DistilBertModel
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification


import json
import random
import matplotlib.pyplot as plt
import cv2
from PIL import Image


import urllib.request
import os
import ssl
ssl._create_default_https_context = ssl._create_unverified_context
from tqdm import tqdm

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import transforms

import torchvision.models as models

annotations = json.load(open("v2_mscoco_train2014_annotations.json"))
questions = json.load(open("v2_OpenEnded_mscoco_train2014_questions.json"))

annotations.keys()

annotations['annotations'][0]

questions.keys()

questions['questions']

questionid2question = {item['question_id']: item['question'] for item in questions['questions']}

questionid2question

annotations['annotations'][0]

def get_most_probable_answers(answers):
    answers_count = dict()
    for answer in answers:
        # TODO: if answer_confidence == 'yes'
        # if answer['answer_confidence'] != 'yes':
        #     continue

        answers_count[answer['answer']] = answers_count.get(answer['answer'], 0) + 1
    max_answer = max(answers_count, key = answers_count.get)
    return max_answer

def build_dataset(questions, annotations):
    questionid2question = {item['question_id']: item['question'] for item in questions['questions']}
    questions = questions['questions'] # image_id, question, question_id
    annotations = annotations['annotations'] # image_id, question_id, answers
    question = [questionid2question[item['question_id']] for item in annotations]
    image_id = [item['image_id'] for item in annotations]
    answer = [get_most_probable_answers(item['answers']) for item in annotations]
    df = pd.DataFrame(list(zip(image_id, question, answer)), columns =['image_id', 'question', 'answer'])
    return df

df = build_dataset(questions, annotations)

df

all_possible_answers = df['answer'].unique()
len(all_possible_answers)

threshold_instance_per_class = 1000 # 1000

# threshold_num_classes = 20 # 20
threshold_num_classes = 5

sample_size = 1000 # 1000

def choose_classes_strategy(filterd_answers_freq, threshold_num_classes=None, strategy=None):
    strategy = strategy.lower()
    assert strategy in ['max', 'min', 'random', None]

    if strategy is None:
        # choose all classes
        chosen_classes = filterd_answers_freq.keys()
    elif strategy == 'min':
        # choose classes which have minimum number of frequencies
        assert(threshold_num_classes is not None)
        chosen_classes = sorted(filterd_answers_freq, key=lambda x:filterd_answers_freq[x])[0:threshold_num_classes]
    elif strategy == 'max':
        # choose classes which have maximum number of frequencies
        assert(threshold_num_classes is not None)
        chosen_classes = sorted(filterd_answers_freq, key=lambda x:filterd_answers_freq[x], reverse=True)[0:threshold_num_classes]
    elif strategy == 'random':
        # choose classes randomly
        assert(threshold_num_classes is not None)
        chosen_classes = random.sample(list(filterd_answers_freq.keys()), threshold_num_classes)

    return chosen_classes


def filter_dataframe(df, chosen_classes, sample_size):
    # TODO: sampling from chosen classed
    # raise NotImplementedError
    sampled_rows = []
    for cls in chosen_classes:
        class_rows = df[df['answer'] == cls]
        class_sample = class_rows.sample(min(sample_size, len(class_rows)))
        sampled_rows.append(class_sample)
    return pd.concat(sampled_rows)



def get_final_df(df, threshold_instance_per_class=None, threshold_num_classes=None, sample_size=None):
    ans_freq = df['answer'].value_counts()
    filterd_answers_freq = ans_freq[ans_freq > threshold_instance_per_class]
    chosen_classes = choose_classes_strategy(filterd_answers_freq, threshold_num_classes, strategy='random')
    filtered_df = filter_dataframe(df, chosen_classes, sample_size)
    return filtered_df

new_df = get_final_df(df, threshold_instance_per_class, threshold_num_classes, sample_size)

new_df

new_df.to_csv("new_df.csv")

new_df = pd.read_csv("new_df.csv", index_col = 0)

def train_val_test_split(df, train_portion = 0.9, val_portion = 0.05, test_portion = 0.05):
    df_train, df_val, df_test, _ = np.split(df.sample(frac=1, random_state=42), [int(train_portion * len(df)), int((train_portion + val_portion) * len(df)), int((train_portion + val_portion + test_portion) * len(df))])
    df_train = df_train.reset_index(drop=True)
    df_val = df_val.reset_index(drop=True)
    df_test = df_test.reset_index(drop=True)
    return df_train, df_val, df_test

df_train, df_val, df_test = train_val_test_split(new_df)
# max_question_len = max(df['question'].apply(lambda x: len(x)))
max_question_len = max(new_df['question'].apply(lambda x: len(x)))
MAX_LEN = 100

len(new_df), len(df_train), len(df_val), len(df_test)

max_question_len

def get_tokenizer_output(tokenizer, text, attr):
    return np.array(tokenizer(text, padding='max_length', max_length = MAX_LEN, truncation=True, return_tensors="pt")[attr])

test_transform = transforms.Compose([
            transforms.Resize((112,112)),
            transforms.ToTensor(),
        ])

train_transform = transforms.Compose([
            transforms.Resize((112,112)),
            # transforms.RandomResizedCrop((112,112)),
            transforms.ToTensor(),
        ])

val_transform = transforms.Compose([
            transforms.Resize((112,112)),
            transforms.ToTensor(),
        ])


class SimpleVQADataset(Dataset):
    def __init__(self, df, tokenizer, classes, transform):
        super(SimpleVQADataset, self).__init__()
        self.df = df
        self.tokenizer = tokenizer
        self.labels = []
        self.label2idx = {c:i for i, c in enumerate(classes)}
        self.idx2label = {i:c for i, c in enumerate(classes)}
        self.mapping = self.cunstruct_mapping_dict()
        self.encodings = {}
        self.encodings['input_ids']=[]
        self.encodings['attention_mask']=[]
        self.transform = transform

        for index, row in df.iterrows():
            self.encodings['input_ids'].append(get_tokenizer_output(self.tokenizer, row['question'], 'input_ids'))
            self.encodings['attention_mask'].append(get_tokenizer_output(self.tokenizer, row['question'], 'attention_mask'))
            self.labels.append(self.label2idx[self.mapping[row['answer'].lower()]])
        self.encodings['input_ids'] = np.array(self.encodings['input_ids'])
        self.encodings['attention_mask'] = np.array(self.encodings['attention_mask'])

        self.img_ids = df['image_id']
        self.labels = np.array(self.labels).reshape(-1, 1)


    def __len__(self):
        return len(self.labels)


    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        image_id = self.img_ids[idx]
        item['image_id'] = image_id
        filename = 'COCO_train2014_'+ str(image_id).zfill(12) + '.jpg'
        trg = os.path.join('./images', filename)
        img = self.transform(Image.open(trg).convert('RGB'))
        item['image'] = img

        return item

    def cunstruct_mapping_dict(self, ):
        map_dict = {}
        for item in self.idx2label.values():
            map_dict[item.lower()] = item
        return map_dict

tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

classes = new_df['answer'].unique()
classes

train_dataset = SimpleVQADataset(df_train, tokenizer, classes, train_transform)
val_dataset = SimpleVQADataset(df_val, tokenizer, classes, val_transform)
test_dataset = SimpleVQADataset(df_test, tokenizer, classes, test_transform)


print("Ù”Number of samples in train split", len(train_dataset))
print("Number of samples in val split", len(val_dataset))
print("Number of samples in test split", len(test_dataset))

train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)
val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4)
test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)

import os
import urllib.request
from concurrent.futures import ThreadPoolExecutor
from tqdm import tqdm
import matplotlib.pyplot as plt

target_folder = './'
subfolder_name = 'images'
subfolder_path = os.path.join(target_folder, subfolder_name)

if not os.path.exists(subfolder_path):
    os.makedirs(subfolder_path)

unique_image_ids = new_df['image_id'].unique()

def download_image(img_id):
    filename = 'COCO_train2014_'+ str(img_id).zfill(12) + '.jpg'
    src = os.path.join('https://vqa_mscoco_images.s3.amazonaws.com/train2014/', filename)
    trg = os.path.join(subfolder_path, filename)
    if not os.path.exists(trg):
        urllib.request.urlretrieve(src, trg)

# Using ThreadPoolExecutor to concurrently download images
with ThreadPoolExecutor() as executor:
    list(tqdm(executor.map(download_image, unique_image_ids), total=len(unique_image_ids)))

"""# VQA Model"""

# Step 3: Define the VQA model
class VQAModel(nn.Module):
    def __init__(self, image_feature_extractor, question_feature_extractor, hidden_dim, num_classes):
        super(VQAModel, self).__init__()
        # Image feature extractor (e.g., ResNet50)
        self.image_feature_extractor = image_feature_extractor
        # Question feature extractor (e.g., LSTM)
        self.question_feature_extractor = question_feature_extractor

        # Combined image and question classifier
        self.classifier = nn.Sequential(
            # Define the layers that combine image and question features
            # ...
            nn.Linear(hidden_dim, hidden_dim),
            nn.Dropout(p=0.2),
            nn.ReLU(),
            nn.Linear(hidden_dim, num_classes)

        )

    def forward(self, image, input_ids, attention_mask):
        # Process the image through the image_features
        image_embedding = self.image_feature_extractor(image).squeeze((2, 3))
        # print(image_embedding.size())

        # Process the question through the question_features
        # question_embedding = self.question_feature_extractor(**question).last_hidden_state.mean(dim=1)
        question_embedding = self.question_feature_extractor(input_ids=input_ids,attention_mask=attention_mask).last_hidden_state.mean(dim=1)
        # print(question_embedding.size())


        # Combine the image and question features
        # combined_embedding = torch.cat((image_embedding, question_embedding[:, -1, :]), dim=1)
        combined_embedding = torch.cat((image_embedding, question_embedding), dim=1)

        # Make a prediction using the classifier
        output = self.classifier(combined_embedding)
        return output

# Step 4: Train the model
def train_model(model, train_loader, criterion, optimizer, num_epochs):
    model.train()
    for epoch in range(num_epochs):
        total_loss = 0.0
        total_correct = 0
        total_samples = 0
        for train_batch in tqdm(train_loader):
            images = train_batch['image'].squeeze(1).to(device)
            input_ids = train_batch['input_ids'].squeeze(1).to(device).to(device)
            attention_mask = train_batch['attention_mask'].squeeze(1).to(device)
            train_label = train_batch['labels'].squeeze(1).to(device)

            # Forward pass
            outputs = model(images, input_ids, attention_mask)

            _, preds = torch.max(outputs, dim=1)
            # Compute the loss
            loss = criterion(outputs, train_label)
            # Backpropagation
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
            total_correct += (preds == train_label).sum().item()
            total_samples += train_label.size(0)
            # Clear the gradients
            optimizer.zero_grad()
        # Print the loss for this epoch
        print(f"Train, Epoch {epoch + 1} : Loss = {total_loss / len(train_dataloader):.4f}, Accuracy = {total_correct / total_samples:.4f}")

# Step 5: Evaluate the model
def evaluate_model(model, val_loader, criterion, optimizer):
    model.eval()
    correct = 0
    total = 0
    total_loss = 0.0
    total_correct = 0
    total_samples = 0

    with torch.no_grad():
        for val_batch in tqdm(val_loader):
            images = val_batch['image'].squeeze(1).to(device)
            input_ids = val_batch['input_ids'].squeeze(1).to(device).to(device)
            attention_mask = val_batch['attention_mask'].squeeze(1).to(device)
            val_label = val_batch['labels'].squeeze(1).to(device)

            # Forward pass
            outputs = model(images, input_ids, attention_mask)
            # Get the predicted answers
            _, preds = torch.max(outputs, dim=1)
            # Update total and correct predictions
            total_correct += (preds == val_label).sum().item()
            total_samples += val_label.size(0)

    print(f"Evaluation : Loss = {total_loss / len(val_dataloader):.4f}, Accuracy = {total_correct / total_samples:.4f}")

classes # !!! check is 5

if __name__ == "__main__":
    # Set device (CPU or GPU)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


    # image_feature_extractor, question_feature_extractor, hidden_dim, num_classes
    # hidden_dim = 512 + 768    # Replace with the desired hidden dimension
    hidden_dim = 2048 + 768    # Replace with the desired hidden dimension

    # model_ft = models.resnet18(pretrained=True)
    model_ft = models.resnet50(pretrained=True)

    image_feature_extractor = torch.nn.Sequential(*list(model_ft.children())[:-1])

    # freeze resnet50
    for param in image_feature_extractor.parameters():
        param.requires_grad = False

    question_feature_extractor = DistilBertModel.from_pretrained('distilbert-base-uncased')

    num_classes = len(classes)

    model = VQAModel(image_feature_extractor, question_feature_extractor, hidden_dim, num_classes).to(device)

    criterion = nn.CrossEntropyLoss()

    # optimizer = optim.Adam(model.parameters(), lr=0.001)
    optimizer = optim.AdamW(model.parameters(), lr=0.001)

    scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.001, max_lr=0.1, step_size_up=71, mode="exp_range", cycle_momentum=False)



    num_epochs = 10



    for epoch in range(num_epochs):
        model.train()
        total_loss = 0.0
        total_correct = 0
        total_samples = 0
        for train_batch in tqdm(train_dataloader):
            images = train_batch['image'].squeeze(1).to(device)
            input_ids = train_batch['input_ids'].squeeze(1).to(device).to(device)
            attention_mask = train_batch['attention_mask'].squeeze(1).to(device)
            train_label = train_batch['labels'].squeeze(1).to(device)

            # Forward pass
            outputs = model(images, input_ids, attention_mask)

            _, preds = torch.max(outputs, dim=1)
            # Compute the loss
            loss = criterion(outputs, train_label)
            # Backpropagation
            loss.backward()
            optimizer.step()
            scheduler.step()

            total_loss += loss.item()
            total_correct += (preds == train_label).sum().item()
            total_samples += train_label.size(0)
            # Clear the gradients
            optimizer.zero_grad()
        print(f"Train, Epoch {epoch + 1} : Loss = {total_loss / len(train_dataloader):.4f}, Accuracy = {total_correct / total_samples:.4f}")


        model.eval()
        correct = 0
        total = 0
        total_loss = 0.0
        total_correct = 0
        total_samples = 0

        # with torch.no_grad():
        for val_batch in tqdm(val_dataloader):
            images = val_batch['image'].squeeze(1).to(device)
            input_ids = val_batch['input_ids'].squeeze(1).to(device).to(device)
            attention_mask = val_batch['attention_mask'].squeeze(1).to(device)
            val_label = val_batch['labels'].squeeze(1).to(device)

            # Forward pass
            outputs = model(images, input_ids, attention_mask)
            # Get the predicted answers
            _, preds = torch.max(outputs, dim=1)
            loss = criterion(outputs, val_label)
            # Update total and correct predictions
            total_loss += loss.item()
            total_correct += (preds == val_label).sum().item()
            total_samples += val_label.size(0)

        print(f"Evaluation, Epoch {epoch + 1} : Loss = {total_loss / len(val_dataloader):.4f}, Accuracy = {total_correct / total_samples:.4f}")

evaluate_model(model, test_dataloader, criterion, optimizer)